---
title: "ch8exercises"
author: "Lauren Temple"
date: "2/24/2022"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
pacman::p_load(ISLR2, tree, randomForest, gbm, BART, caTools)
```

8.1 Draw an example (of your own invention) of a partition of two-dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all aspects of your figures, including the regions R1, R2,..., the cutpoints t1, t2,..., and so forth.

```{r}
knitr::include_graphics("8.1.jpg")
```

8.2 It is mentioned in Section 8.2.3 that boosting using depth-one trees (or stumps) leads to an additive model, Explain why this is the case. You can begin with (8.12) in Algorithm 8.2.

This is because each model consists of a single split using one distinct variable. Therefore the total number of decision trees is the same as the number of predictors. A new model is fit on the residuals of the previous model and the new models output is then added to the previous models, making the final model additive.

8.3 Consider the Gini index, classification error, and entropy in a simple classification setting with two classes. Create a single plot that displays each of these quantities as a function of ˆpm1. The x-axis should display ˆpm1, ranging from 0 to 1, and the y-axis should display the value of the Gini index, classification error, and entropy. Hint: In a setting with two classes, pˆm1 = 1 − pˆm2. You could make this plot by hand, but it will be much easier to make in R.
```{r}
#classification error
p1 <- seq(0, 1, 0.01)
E1 <- 1-p1[51:101]
E2 <- 1-(1-p1[1:51])

plot(1, type="n", main="Gini Index, Classification Error and Cross-Entropy",
     xlab=expression(hat(p)[m1]), ylab="Values", xlim=seq(0,1), ylim=c(0, 1))
points(x=p1[1:51], y = c(E2), type = "l", lwd=2)
points(x=p1[51:101], y = c(E1), type = "l", lwd=2)

#Gini Index
G <- 2*pi*(1-p1)
lines(p1, G, col="blue", lwd= 2)

#Cross Entropy
D <- -p1*log(p1)-(1-p1)*log(1-p1)
lines(p1, D, col="red", lwd= 2)

legend(0.7, 0.9, legend= c("Classification Error", "Gini Index", "Cross-Entropy"), 
       col= c("black", "blue", "red"), lty= c(1,1,1), lwd=c(2,2,2))

```

8.5 Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of X, produce 10 estimates of P(Class is Red|X): 0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75. There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability. In this example, what is the final classification under each of these two approaches?

Majority Voting:
P(Class is Red | X) < 0.5 = 4 
P(Class is Red | X) > 0.5 = 6
X is classified as red

Average probability:
The average P(Class is Red | X) is 4.5/10= 0.45
X is classified as green

8.7 In the lab, we applied random forests to the Boston data using mtry = 6 and using ntree = 25 and ntree = 500. Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for mtry and ntree. You can model your plot after Figure 8.10. Describe the results obtained.
```{r}
set.seed(42)
data(Boston)
df <- Boston
sample.data <- sample.split(df$medv, SplitRatio= 0.70)
train.set <- subset(df, select= -c(medv), sample.data==T) #drop the medv column
test.set <- subset(df, select=-c(medv), sample.data==F)
train.Y <- subset(df$medv, sample.data==T)
test.Y <- subset(df$medv, sample.data==F)
```

```{r}
#four random forest models
p <- 13
rf1 <- randomForest(train.set, train.Y, test.set, test.Y, mtry= p, ntree= 700)

rf2 <- randomForest(train.set, train.Y, test.set, test.Y, mtry= p/2, ntree= 700)

rf3 <- randomForest(train.set, train.Y, test.set, test.Y, mtry= p/3, ntree= 700)

rf4 <- randomForest(train.set, train.Y, test.set, test.Y, mtry= p/4, ntree= 700)
```

```{r}
x.axis <- seq(1, 700, 1)
plot(x.axis, rf1$test$mse, xlab= "Number of Trees", ylab= "Test Error", ylim= c(5,20), type="l", lwd= 2)
lines(x.axis, rf2$test$mse, col="red", lwd=2)
lines(x.axis, rf3$test$mse, col="blue", lwd=2)
lines(x.axis, rf4$test$mse, col="green", lwd=2)
```
The test error decreases as the number of trees increases. Test error gets lower as m decreases from m=p up to m=p/3 and after that there is not much change.

8.8 In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.

a. Split the data set into a training set and a test set.
```{r}
set.seed(42)
data("Carseats")
df <- Carseats
sample.data <- sample.split(df$Sales, SplitRatio=0.70)

train.set <- subset(df, sample.data==T)
test.set <- subset(df, sample.data==F)
```

b. Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?
```{r}
tree.carseats <- tree(Sales ~., data= train.set)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
```

```{r}
#test mse
tree.pred <- predict(tree.carseats, test.set)
test.mse <- mean((tree.pred-test.set$Sales)^2)
test.mse
```
Shelf location and price are important predictors. The test MSE is 3.308

c. Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?
```{r}
set.seed(2)
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, xlab="Terminal Nodes", ylab="CV Error", type="b")
```
The CV Error is lowest for a tree with 9 terminal nodes

```{r}
prune.carseats <- prune.tree(tree.carseats, best= 9)
tree.pred <- predict(prune.carseats, test.set)
test.mse <- mean((tree.pred-test.set$Sales)^2)
test.mse
```
I do not see the test mse improve

d. Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.
```{r}
set.seed(42)
bag.carseats <- randomForest(Sales ~., data= train.set, mtry= 10, importance=T)
importance(bag.carseats)
bag.yhat <- predict(bag.carseats, newdata= test.set)
mean((bag.yhat-test.set$Sales)^2)
```
The most important variables are price and shelf location as we saw previously. The test MSE is 1.99 which is improved from teh random forest method.

e. Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.
```{r}

```

f. Now analyze the data using BART, and report your results.
```{r}

```


8.11 Caravan data set
a. Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.
```{r}

```

b. Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?
```{r}

```

c. Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 %. Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?
```{r}

```

